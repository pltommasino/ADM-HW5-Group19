{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fb6cf567",
      "metadata": {
        "id": "fb6cf567"
      },
      "source": [
        "# Homework 5 - The eternal significance of publications and citations!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45fcc293",
      "metadata": {
        "id": "45fcc293"
      },
      "source": [
        "This project was carried out by Group 19 of Algorithmic Methods for Data Mining, consisting of:\n",
        "\n",
        "| NAME and SURNAME | EMAIL |\n",
        "| --- | --- |\n",
        "| Pasquale Luca Tommasino | pl.tommasino@gmail.com | \n",
        "| Deniz Yilmaz | denizyilmazz@yahoo.com |\n",
        "| Emmanuele De Lucia | delucia.2099678@studenti.uniroma1.it |\n",
        "| Paolo Zilviano | zilviano.1916518@studenti.uniroma1.it |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bd6c038",
      "metadata": {
        "id": "1bd6c038"
      },
      "source": [
        "## 1. Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "611fcb89",
      "metadata": {
        "id": "611fcb89"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import ijson\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import random\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cddd8d36",
      "metadata": {
        "id": "cddd8d36"
      },
      "source": [
        "At the first, let's see how the file is structured, let's take the first row of the dataset and see what the columns and the first row of the dataset are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "14daa051",
      "metadata": {
        "id": "14daa051"
      },
      "outputs": [],
      "source": [
        "with open('dblp.v12.json', 'r') as file:\n",
        "    # Parse the JSON array items one by one\n",
        "    array_items = ijson.items(file, 'item')\n",
        "    # Iterate over the JSON array items\n",
        "    for item in array_items:\n",
        "        dict1 = item\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "559c93a5",
      "metadata": {
        "id": "559c93a5",
        "outputId": "3bb365a1-9fe6-4bd6-b0ee-39fa3cb99b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['id', 'authors', 'title', 'year', 'n_citation', 'page_start', 'page_end', 'doc_type', 'publisher', 'volume', 'issue', 'doi', 'references', 'indexed_abstract', 'fos', 'venue']\n"
          ]
        }
      ],
      "source": [
        "#List the column of the json file\n",
        "chiave = list(dict1.keys())\n",
        "print(chiave)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ec97c927",
      "metadata": {
        "id": "ec97c927",
        "outputId": "c1bd9f4f-cfc5-4535-e824-cca1cdadc190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1091, [{'name': 'Makoto Satoh', 'org': 'Shinshu University', 'id': 2312688602}, {'name': 'Ryo Muramatsu', 'org': 'Shinshu University', 'id': 2482909946}, {'name': 'Mizue Kayama', 'org': 'Shinshu University', 'id': 2128134587}, {'name': 'Kazunori Itoh', 'org': 'Shinshu University', 'id': 2101782692}, {'name': 'Masami Hashimoto', 'org': 'Shinshu University', 'id': 2114054191}, {'name': 'Makoto Otani', 'org': 'Shinshu University', 'id': 1989208940}, {'name': 'Michio Shimizu', 'org': 'Nagano Prefectural College', 'id': 2134989941}, {'name': 'Masahiko Sugimoto', 'org': 'Takushoku University, Hokkaido Junior College', 'id': 2307479915}], 'Preliminary Design of a Network Protocol Learning Tool Based on the Comprehension of High School Students: Design by an Empirical Study Using a Simple Mind Map', 2013, 1, '89', '93', 'Conference', 'Springer, Berlin, Heidelberg', '', '', '10.1007/978-3-642-39476-8_19', [2005687710, 2018037215], {'IndexLength': 58, 'InvertedIndex': {'tool.': [42], 'study': [4], 'aim': [37], 'purpose': [1], 'scientific': [17], 'for': [11], 'aspects': [18], 'students': [14, 46], 'focus': [27], 'hands-on': [47], 'learning': [9, 41], 'experience': [48], 'our': [40], 'we': [26], 'network': [33, 56], 'The': [0], 'More': [24], 'high': [12], 'protocols.': [57], 'school': [13], 'and': [21], 'of': [2, 19, 32, 55], 'communication': [22], 'protocols': [34], 'gives': [45], 'on': [28], 'a': [8], 'studying': [15], 'specifically,': [25], 'this': [3], 'understand': [51], 'is': [5], 'develop': [7, 39], 'Our': [43], 'tool': [10, 44], 'the': [16, 29, 36, 52], 'help': [50], 'as': [35], 'principles': [31, 54], 'information': [20], 'networks.': [23], 'to': [6, 38, 49], 'basic': [30, 53]}}, [{'name': 'Telecommunications network', 'w': Decimal('0.45139')}, {'name': 'Computer science', 'w': Decimal('0.45245')}, {'name': 'Mind map', 'w': Decimal('0.5347')}, {'name': 'Human–computer interaction', 'w': Decimal('0.47011')}, {'name': 'Multimedia', 'w': Decimal('0.46629')}, {'name': 'Empirical research', 'w': Decimal('0.49737')}, {'name': 'Comprehension', 'w': Decimal('0.47042')}, {'name': 'Communications protocol', 'w': Decimal('0.51907')}], {'raw': 'International Conference on Human-Computer Interaction', 'id': 1127419992, 'type': 'C'}]\n"
          ]
        }
      ],
      "source": [
        "#List of values of first row of the json file\n",
        "valori = list(dict1.values())\n",
        "print(valori)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a20b7f72",
      "metadata": {
        "id": "a20b7f72"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1ae8a3",
      "metadata": {
        "id": "5e1ae8a3"
      },
      "source": [
        "We now transform the entire *.json file* into a dictionary with selected columns (*id, authors, title, year, n_citation, doc_type, publisher* and *references*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "323011fd",
      "metadata": {
        "id": "323011fd"
      },
      "outputs": [],
      "source": [
        "#Create a dict for Collaboration Graph\n",
        "dict_entire = {\n",
        "    'id' : [],\n",
        "    'authors' : [],\n",
        "    'title' : [],\n",
        "    'year' : [],\n",
        "    'n_citation' : [],\n",
        "    'doc_type' : [],\n",
        "    'publisher' : [],\n",
        "    'references' : []\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc0be070",
      "metadata": {
        "id": "fc0be070"
      },
      "source": [
        "We create functions for processing the *.json file* (they are in **function.py**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b62e02db",
      "metadata": {
        "id": "b62e02db",
        "outputId": "16c208b1-d28c-404d-b10f-fbbda51b2781"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4894081it [06:03, 13463.48it/s]\n"
          ]
        }
      ],
      "source": [
        "sys.path.append(\"dict\")\n",
        "from dictionary import *\n",
        "\n",
        "with open('dblp.v12.json', 'r') as file:\n",
        "    # Parse the JSON array items one by one\n",
        "    array_items = ijson.items(file, 'item')\n",
        "    # Iterate over the JSON array items\n",
        "    for item in tqdm(array_items):\n",
        "        # Process each JSON array item as needed\n",
        "        dict_id(dict_entire, item)\n",
        "        dict_authors(dict_entire, item)\n",
        "        dict_title(dict_entire, item)\n",
        "        dict_year(dict_entire, item)\n",
        "        dict_n_citation(dict_entire, item)\n",
        "        dict_doc_type(dict_entire, item)\n",
        "        dict_publisher(dict_entire, item)\n",
        "        dict_references(dict_entire, item)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f639ec06",
      "metadata": {
        "id": "f639ec06"
      },
      "source": [
        "Transform dictionary in Pandas Dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0bb0d247",
      "metadata": {
        "id": "0bb0d247",
        "outputId": "2510f3ca-53e1-40a7-ed2d-a403ba80099e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>authors</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "      <th>n_citation</th>\n",
              "      <th>doc_type</th>\n",
              "      <th>publisher</th>\n",
              "      <th>references</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1091</td>\n",
              "      <td>[Makoto Satoh, Ryo Muramatsu, Mizue Kayama, Ka...</td>\n",
              "      <td>Preliminary Design of a Network Protocol Learn...</td>\n",
              "      <td>2013</td>\n",
              "      <td>1</td>\n",
              "      <td>Conference</td>\n",
              "      <td>Springer, Berlin, Heidelberg</td>\n",
              "      <td>[2005687710, 2018037215]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1388</td>\n",
              "      <td>[Pranava K. Jha]</td>\n",
              "      <td>Further Results on Independence in Direct-Prod...</td>\n",
              "      <td>2000</td>\n",
              "      <td>1</td>\n",
              "      <td>Journal</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1674</td>\n",
              "      <td>[G. Beale, G. Earl]</td>\n",
              "      <td>A methodology for the physically accurate visu...</td>\n",
              "      <td>2011</td>\n",
              "      <td>1</td>\n",
              "      <td>Conference</td>\n",
              "      <td>Eurographics Association</td>\n",
              "      <td>[1535888970, 1992876689, 1993710814, 203565334...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1688</td>\n",
              "      <td>[Altaf Hossain, Faisal Zaman, M. Nasser, M. Mu...</td>\n",
              "      <td>Comparison of GARCH, Neural Network and Suppor...</td>\n",
              "      <td>2009</td>\n",
              "      <td>6</td>\n",
              "      <td>Conference</td>\n",
              "      <td>Springer, Berlin, Heidelberg</td>\n",
              "      <td>[1560724230, 1986968751, 2156909104]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5411</td>\n",
              "      <td>[Rafael Álvarez, Leandro Tortosa, José-Francis...</td>\n",
              "      <td>COMPARING GNG3D AND QUADRIC ERROR METRICS METH...</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>Conference</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                                            authors  \\\n",
              "0  1091  [Makoto Satoh, Ryo Muramatsu, Mizue Kayama, Ka...   \n",
              "1  1388                                   [Pranava K. Jha]   \n",
              "2  1674                                [G. Beale, G. Earl]   \n",
              "3  1688  [Altaf Hossain, Faisal Zaman, M. Nasser, M. Mu...   \n",
              "4  5411  [Rafael Álvarez, Leandro Tortosa, José-Francis...   \n",
              "\n",
              "                                               title  year  n_citation  \\\n",
              "0  Preliminary Design of a Network Protocol Learn...  2013           1   \n",
              "1  Further Results on Independence in Direct-Prod...  2000           1   \n",
              "2  A methodology for the physically accurate visu...  2011           1   \n",
              "3  Comparison of GARCH, Neural Network and Suppor...  2009           6   \n",
              "4  COMPARING GNG3D AND QUADRIC ERROR METRICS METH...  2009           0   \n",
              "\n",
              "     doc_type                     publisher  \\\n",
              "0  Conference  Springer, Berlin, Heidelberg   \n",
              "1     Journal                          None   \n",
              "2  Conference      Eurographics Association   \n",
              "3  Conference  Springer, Berlin, Heidelberg   \n",
              "4  Conference                          None   \n",
              "\n",
              "                                          references  \n",
              "0                           [2005687710, 2018037215]  \n",
              "1                                               None  \n",
              "2  [1535888970, 1992876689, 1993710814, 203565334...  \n",
              "3               [1560724230, 1986968751, 2156909104]  \n",
              "4                                               None  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Transform dict in dataframe\n",
        "df_entire = pd.DataFrame(dict_entire)\n",
        "df_entire.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "81fe7365",
      "metadata": {
        "id": "81fe7365"
      },
      "outputs": [],
      "source": [
        "#Save df in csv file\n",
        "#df_entire.to_csv('df_entire.csv')\n",
        "#Read csv df file\n",
        "#df_entire = pd.read_csv('df_entire.csv', index_col=0, low_memory=False)\n",
        "\n",
        "#Drop NaN value in the column I\n",
        "#df_entire = df_entire.dropna(subset='id')\n",
        "\n",
        "#Transform columns from decimal to integer\n",
        "#df_entire['id'] = df_entire['id'].astype(int)\n",
        "#df_entire['n_citation'] = df_entire['n_citation'].astype(int)\n",
        "#Transform column from integer to datetime (year)\n",
        "#df_entire['year'] = df_entire['year'].astype('datetime64[Y]')\n",
        "\n",
        "#See the 'head' of dataset\n",
        "#df_entire.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9bca0d8",
      "metadata": {
        "id": "f9bca0d8"
      },
      "source": [
        "#### 1. Identify the top 10,000 papers with the highest number of citations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0ae167f0",
      "metadata": {
        "id": "0ae167f0",
        "outputId": "b4144b46-98bf-4762-dc69-ccbd02f9dbf8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>authors</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "      <th>n_citation</th>\n",
              "      <th>doc_type</th>\n",
              "      <th>publisher</th>\n",
              "      <th>references</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4696136</th>\n",
              "      <td>2041404167</td>\n",
              "      <td>[C. E. Shannon]</td>\n",
              "      <td>The Mathematical Theory of Communication</td>\n",
              "      <td>1949</td>\n",
              "      <td>48327</td>\n",
              "      <td>Book</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4630907</th>\n",
              "      <td>1639032689</td>\n",
              "      <td>[David E. Goldberg]</td>\n",
              "      <td>Genetic algorithms in search, optimization, an...</td>\n",
              "      <td>1989</td>\n",
              "      <td>44175</td>\n",
              "      <td>Book</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4092588</th>\n",
              "      <td>2912565176</td>\n",
              "      <td>[Lotfi A. Zadeh]</td>\n",
              "      <td>Fuzzy sets</td>\n",
              "      <td>1996</td>\n",
              "      <td>42437</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2937610</th>\n",
              "      <td>2151103935</td>\n",
              "      <td>[David G. Lowe]</td>\n",
              "      <td>Distinctive Image Features from Scale-Invarian...</td>\n",
              "      <td>2004</td>\n",
              "      <td>35541</td>\n",
              "      <td>Journal</td>\n",
              "      <td>Kluwer Academic Publishers</td>\n",
              "      <td>[19720318, 1541642243, 1560959218, 1676552347,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4088311</th>\n",
              "      <td>2911964244</td>\n",
              "      <td>[Leo Breiman]</td>\n",
              "      <td>Random Forests</td>\n",
              "      <td>2001</td>\n",
              "      <td>34741</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>[1507255258, 1580948147, 1605688901, 197584664...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id              authors  \\\n",
              "4696136  2041404167      [C. E. Shannon]   \n",
              "4630907  1639032689  [David E. Goldberg]   \n",
              "4092588  2912565176     [Lotfi A. Zadeh]   \n",
              "2937610  2151103935      [David G. Lowe]   \n",
              "4088311  2911964244        [Leo Breiman]   \n",
              "\n",
              "                                                     title  year  n_citation  \\\n",
              "4696136           The Mathematical Theory of Communication  1949       48327   \n",
              "4630907  Genetic algorithms in search, optimization, an...  1989       44175   \n",
              "4092588                                         Fuzzy sets  1996       42437   \n",
              "2937610  Distinctive Image Features from Scale-Invarian...  2004       35541   \n",
              "4088311                                     Random Forests  2001       34741   \n",
              "\n",
              "        doc_type                   publisher  \\\n",
              "4696136     Book                        None   \n",
              "4630907     Book                        None   \n",
              "4092588     None                        None   \n",
              "2937610  Journal  Kluwer Academic Publishers   \n",
              "4088311     None                        None   \n",
              "\n",
              "                                                references  \n",
              "4696136                                               None  \n",
              "4630907                                               None  \n",
              "4092588                                               None  \n",
              "2937610  [19720318, 1541642243, 1560959218, 1676552347,...  \n",
              "4088311  [1507255258, 1580948147, 1605688901, 197584664...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Create the top 10000 papers with the highest number of citations\n",
        "df_10th_papers = df_entire.sort_values(by=['n_citation'], ascending=False)[:10000]\n",
        "\n",
        "#See the first five lines\n",
        "df_10th_papers.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a9db47c9",
      "metadata": {
        "id": "a9db47c9"
      },
      "outputs": [],
      "source": [
        "#Save df in csv file\n",
        "#df_10th_papers.to_csv('df_10th_papers.csv')\n",
        "#Read csv df file\n",
        "#df_10th_papers = pd.read_csv('df_10th_papers.csv')\n",
        "\n",
        "#Transform columns from decimal to integer\n",
        "#df_10th_papers['id'] = df_10th_papers['id'].astype(int)\n",
        "#df_10th_papers['n_citation'] = df_10th_papers['n_citation'].astype(int)\n",
        "#Transform column from integer to datetime (year)\n",
        "#df_10th_papers['year'] = df_10th_papers['year'].astype('datetime64[Y]')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bf14f33",
      "metadata": {
        "id": "4bf14f33"
      },
      "source": [
        "#### 2. Then the nodes of your graphs would be as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71a48f48",
      "metadata": {
        "id": "71a48f48"
      },
      "source": [
        "##### - **Citation graph**: you can consider each of the papers as your nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "16733d7b",
      "metadata": {
        "id": "16733d7b",
        "outputId": "257db8c4-cf6e-4fbb-f443-12929007df3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Chech if in the column 'id' there are any NaN value in column\n",
        "isthere_nan = df_entire['id'].isna().any()\n",
        "isthere_nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6150f2bf",
      "metadata": {
        "id": "6150f2bf",
        "outputId": "19c99b19-2098-4e99-bead-d59029a54cbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1091, 1388, 1674, 1688, 5411]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Create nodes for Citation Graph\n",
        "citationG_nodes = dict_entire['id']\n",
        "citationG_nodes[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2483b51f",
      "metadata": {
        "id": "2483b51f"
      },
      "source": [
        "##### - **Collaboration graph**: the authors of these papers would be your nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "277eac9d",
      "metadata": {
        "id": "277eac9d",
        "outputId": "744ed861-33a1-44c0-ae70-54184fa739d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Chech if in the column 'authors' there are any NaN value in column\n",
        "isthere_nan = df_entire['authors'].isna().any()\n",
        "isthere_nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8cb80421",
      "metadata": {
        "id": "8cb80421"
      },
      "outputs": [],
      "source": [
        "df_entire = df_entire.dropna(subset='authors')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e47bca44",
      "metadata": {
        "id": "e47bca44",
        "outputId": "6e944212-577a-45d5-948e-c55662353ff4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Makoto Satoh',\n",
              " 'Ryo Muramatsu',\n",
              " 'Mizue Kayama',\n",
              " 'Kazunori Itoh',\n",
              " 'Masami Hashimoto']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Create nodes for Collaboration Graph\n",
        "collaborationG_nodes_2 = df_entire['authors']\n",
        "\n",
        "#Unzip the nested lists in one list of authors\n",
        "collaborationG_nodes = []\n",
        "\n",
        "for list_1 in collaborationG_nodes_2:\n",
        "    for author in list_1:\n",
        "        collaborationG_nodes.append(author)\n",
        "\n",
        "collaborationG_nodes[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34f69c4f",
      "metadata": {
        "id": "34f69c4f"
      },
      "source": [
        "#### 3. For the edges of the two graphs, you would have the following cases:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd6e7795",
      "metadata": {
        "id": "fd6e7795"
      },
      "source": [
        "##### - **Citation graph**: only consider the citation relationship between these 10,000 papers and ignore the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7546adcc",
      "metadata": {
        "id": "7546adcc",
        "outputId": "303fd0eb-8846-47db-9c58-804c9d1d339b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(2151103935, 19720318),\n",
              " (2151103935, 1541642243),\n",
              " (2151103935, 1560959218),\n",
              " (2151103935, 1676552347),\n",
              " (2151103935, 1681491849)]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Take a subset of the 10,000 papers dataset, and transform it in a readable dataset...\n",
        "##... with only two column, within one value\n",
        "citationG_edges_df_2 = df_10th_papers.reset_index()[['id','references']]\n",
        "citationG_edges_df_2 = citationG_edges_df_2.explode('references')\n",
        "citationG_edges_df_2 = citationG_edges_df_2.dropna(subset=['references']).reset_index()[['id','references']]\n",
        "\n",
        "#Transform dataset in a list of tuple\n",
        "citationG_edges = [tuple(x) for x in citationG_edges_df_2.to_records(index=False)]\n",
        "citationG_edges[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c83969ee",
      "metadata": {
        "id": "c83969ee"
      },
      "source": [
        "##### - **Collaboration graph**: only consider the collaborations between the authors of these 10,000 papers and ignore the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1574a3f6",
      "metadata": {
        "id": "1574a3f6"
      },
      "outputs": [],
      "source": [
        "collaboration_dict = {\n",
        "    'name1' : [],\n",
        "    'name2' : []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9383d665",
      "metadata": {
        "id": "9383d665",
        "outputId": "691437fd-5210-4732-ccb1-417a670844e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('David W. Hosmer', 'Stanley Lemeshow'),\n",
              " ('Chih-Chung Chang', 'Chih-Jen Lin'),\n",
              " ('Corinna Cortes', 'Vladimir Vapnik'),\n",
              " ('Heng Li', 'Richard Durbin'),\n",
              " ('K. Deb', 'A. Pratap')]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Take a subset of the 10,000 papers dataset, and transform it in a readable dataset...\n",
        "##... with only two column, within one value\n",
        "collaborationG_edges_df_2 = df_10th_papers.reset_index()[['authors']]\n",
        "nestedlist_auth_collaboration = collaborationG_edges_df_2['authors'].tolist()\n",
        "\n",
        "#Create all possible matches of collaboration for each book\n",
        "collaborationG_edges = []\n",
        "for authors_list in nestedlist_auth_collaboration:\n",
        "    for i in range(len(authors_list)):\n",
        "        for j in range(i + 1, len(authors_list)):\n",
        "            collaborationG_edges.append((authors_list[i], authors_list[j]))\n",
        "\n",
        "collaborationG_edges[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a4eeca79",
      "metadata": {
        "id": "a4eeca79"
      },
      "outputs": [],
      "source": [
        "#Create a counter for couple\n",
        "collaborationG_edges_counter = Counter(collaborationG_edges)\n",
        "\n",
        "#Create a dictionary for track couple weight\n",
        "collaborationG_edges_weights = {}\n",
        "\n",
        "#Iteract edges and count from dict\n",
        "for edge, count in collaborationG_edges_counter.items():\n",
        "    collaborationG_edges_weights[edge] = count\n",
        "\n",
        "    #Add weight also for inverted edge\n",
        "    inverted_edge = (edge[1], edge[0])\n",
        "    collaborationG_edges_weights[inverted_edge] = count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f305de48",
      "metadata": {
        "id": "f305de48"
      },
      "source": [
        "#### Now we build the graphs:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2dd61e7",
      "metadata": {
        "id": "e2dd61e7"
      },
      "source": [
        "##### - **Citation graph**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "17676411",
      "metadata": {
        "id": "17676411"
      },
      "outputs": [],
      "source": [
        "#Create a unweighted and directed graph\n",
        "citation_graph = nx.DiGraph()\n",
        "\n",
        "# Add nodes to the graph\n",
        "citation_graph.add_nodes_from(citationG_nodes)\n",
        "# Add edges to the graph\n",
        "citation_graph.add_edges_from(citationG_edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2ffa14ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save in txt\n",
        "#file_path = \"citation_graph.txt\"\n",
        "#nx.write_edgelist(citation_graph, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bd0764f",
      "metadata": {
        "id": "2bd0764f"
      },
      "source": [
        "##### - **Collaboration graph**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "61fc7d97",
      "metadata": {
        "id": "61fc7d97"
      },
      "outputs": [],
      "source": [
        "#Create a weighted and undirected graph\n",
        "collaboration_graph = nx.Graph()\n",
        "\n",
        "#Add nodes to the graph\n",
        "collaboration_graph.add_nodes_from(collaborationG_nodes)\n",
        "\n",
        "#Add edges to the graph\n",
        "for edge, weight in collaborationG_edges_weights.items():\n",
        "    collaboration_graph.add_edge(edge[0], edge[1], weight=weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "0338966d",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save in txt\n",
        "#file_path = \"collaboration_graph.txt\"\n",
        "#nx.write_edgelist(collaboration_graph, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f99b8cd7",
      "metadata": {
        "id": "f99b8cd7"
      },
      "source": [
        "# 2. Controlling system\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd925e2b",
      "metadata": {
        "id": "fd925e2b"
      },
      "source": [
        "### 2.1 Backend Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6c83db1b",
      "metadata": {
        "id": "6c83db1b"
      },
      "outputs": [],
      "source": [
        "#For this section (2.1) we work with the subgraph (10,000) of the entire graph\n",
        "\n",
        "#Shortlist for Citation Subgraph\n",
        "books_red = df_10th_papers['id'].tolist()\n",
        "#Subgraph\n",
        "sub_citation_graph = citation_graph.subgraph(books_red)\n",
        "\n",
        "#Create the shortlist for Collaboration Subgraph\n",
        "auth_red = []\n",
        "for list_2 in nestedlist_auth_collaboration:\n",
        "    for auth in list_2:\n",
        "        auth_red.append(auth)\n",
        "#Subgraph\n",
        "sub_collaboration_graph = collaboration_graph.subgraph(auth_red)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c5c63ca",
      "metadata": {
        "id": "4c5c63ca"
      },
      "source": [
        "#### Functionality 1 - Graph's features\n",
        "\n",
        "**Input**:\n",
        "- The graph\n",
        "- The name of the graph\n",
        "\n",
        "**Output**:\n",
        "- The number of the nodes in the graph\n",
        "- The number of the edges in the graph\n",
        "- The graph density\n",
        "- The graph degree distribution\n",
        "- The average degree of the graph\n",
        "- The graph hubs (hubs are nodes having degrees more extensive than the 95th percentile of the degree distribution)\n",
        "- Whether the graph is dense or sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a4a38c24",
      "metadata": {
        "id": "a4a38c24"
      },
      "outputs": [],
      "source": [
        "sys.path.append(\"funct\")\n",
        "from functionality1 import functionality_1\n",
        "\n",
        "name_graph = 'Sub Citation Graph'\n",
        "len_nodes_citation, len_edges_citation, density_citation, degree_sequence_citation, avg_degree_citation, hubs_citation, classification_graph_citation = functionality_1(sub_citation_graph, name_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1f9e29ce",
      "metadata": {
        "id": "1f9e29ce"
      },
      "outputs": [],
      "source": [
        "sys.path.append(\"funct\")\n",
        "from functionality1 import functionality_1\n",
        "\n",
        "name_graph = 'Sub Collaboration Graph'\n",
        "len_nodes_collab, len_edges_collab, density_collab, degree_sequence_collab, avg_degree_collab, hubs_collab, classification_graph_collab = functionality_1(sub_collaboration_graph, name_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b5fb431",
      "metadata": {
        "id": "2b5fb431"
      },
      "source": [
        "#### Functionality 2 - Nodes' contribution\n",
        "\n",
        "##### Input:\n",
        "- The graph\n",
        "- A node of the graph (paper/author)\n",
        "- The name of the graph\n",
        "\n",
        "##### Output:\n",
        "\n",
        "- The centrality of the node, calculated based on the following centrality measurements:\n",
        "    - Betweeness\n",
        "    - PageRank\n",
        "    - ClosenessCentrality\n",
        "    - DegreeCentrality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3a0f7430",
      "metadata": {
        "id": "3a0f7430"
      },
      "outputs": [],
      "source": [
        "sys.path.append(\"funct\")\n",
        "from functionality2 import functionality_2\n",
        "\n",
        "#node_selected_collaboration = 2155511848\n",
        "\n",
        "name_graph = 'Sub Citation Graph'\n",
        "#node_selected = int(input(\"Insert the Node that the function calculated: \"))\n",
        "betw, pagerank, closeness, degree = functionality_2(sub_citation_graph, node_selected=2155511848, name_graph=name_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faccc51e",
      "metadata": {
        "id": "faccc51e"
      },
      "source": [
        "#### Functionality 3 - Shortest ordered walk\n",
        "\n",
        "##### Input:\n",
        "- The graph data\n",
        "- A sequence of authors_a = [a_2, ..., a_{n-1}]\n",
        "- Initial node a_1 and an end node a_n\n",
        "- *N*: denoting the top *N* authors whose data should be considered\n",
        "\n",
        "##### Output:\n",
        "- The shortest walk of collaborations you need to read to get from author a_1 to author a_n and the papers you need to cross to realize this walk.\n",
        "\n",
        "\n",
        "Considerations:\n",
        "For this functionality, you must implement an algorithm that returns the shortest __walk__ that goes from node a\\_j to a\\_n, which visits **in order** the nodes in _a_. The choice of a\\_j and a\\_n can be made randomly (or if it improves the performance of the algorithm, you can also define it in any other way)\n",
        "\n",
        "__Important Notes:__\n",
        "- This algorithm should be run only on the collaboration graph.\n",
        "- The algorithm needs to handle the case that the graph is not connected. Thus, only some nodes in _a_ are reachable from a\\_1. In such a scenario, it is enough to let the program give in the output the string \"There is no such path.\"\n",
        "- Since we are dealing with walks, you can pass on the same node _a\\_i_ more than once, but you must preserve order. It means you can go back to any author node any time you want, assuming that the order in which you visit the required nodes is still the same.\n",
        "- Once you completed your implementation, ask chatGPT for a different one leveraging another approach in solving the shortest path and prove whether this implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "533d348a",
      "metadata": {
        "id": "533d348a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first node is: Benoit Marchand\n",
            "The last node is: Eri Saijyo\n",
            "The authors sequence is: ['Hideki Tatsukawa', 'Timo Lassmann', 'Andrew P. Gibson', 'Boris R. Jankovic', 'Naoko Suzuki', 'Christian Schmidl', 'Mariko Okada-Hatakeyama', 'Ilya E. Vorontsov']\n",
            "9\n",
            "['Benoit Marchand', 'Hideki Tatsukawa', 'Timo Lassmann', 'Andrew P. Gibson', 'Boris R. Jankovic', 'Naoko Suzuki', 'Christian Schmidl', 'Mariko Okada-Hatakeyama', 'Ilya E. Vorontsov', 'Eri Saijyo']\n"
          ]
        }
      ],
      "source": [
        "sys.path.append(\"funct\")\n",
        "from functionality3 import functionality_3\n",
        "\n",
        "#We selected N = 100\n",
        "N_sel = 100\n",
        "\n",
        "total_shortest_walk, path = functionality_3(sub_collaboration_graph, sequence_authors=0, node_first=0, node_last=0, N=N_sel)\n",
        "\n",
        "print(total_shortest_walk)\n",
        "print(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58bf0b0d",
      "metadata": {},
      "source": [
        "- Once you completed your implementation, ask chatGPT for a different one leveraging another approach in solving the shortest path and prove whether this implementation is correct. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7ba657d5",
      "metadata": {},
      "outputs": [
        {
          "ename": "NodeNotFound",
          "evalue": "Either source Leonard Lipovich or target Yishai Shimoni is not in G",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNodeNotFound\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m     filtered_graph \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39msubgraph(top_nodes)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_graph\n\u001b[0;32m---> 35\u001b[0m result \u001b[38;5;241m=\u001b[39m shortest_ordered_walk(sub_collaboration_graph, authors_a\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeonard Lipovich\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYishai Shimoni\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarco Roncador\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNaoko Takahashi Tagami\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGiuseppe Jurman\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlbin Sandelin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMasanori Suzuki\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinda M. Van Den Berg\u001b[39m\u001b[38;5;124m'\u001b[39m], initial_node\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBoris Lenhard\u001b[39m\u001b[38;5;124m'\u001b[39m, end_node\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnita Schwegmann\u001b[39m\u001b[38;5;124m'\u001b[39m, N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShortest Ordered Walk:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n",
            "Cell \u001b[0;32mIn[28], line 17\u001b[0m, in \u001b[0;36mshortest_ordered_walk\u001b[0;34m(graph, authors_a, initial_node, end_node, N)\u001b[0m\n\u001b[1;32m     14\u001b[0m next_node \u001b[38;5;241m=\u001b[39m authors_a[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Find the shortest path from current_node to next_node\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m path \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mshortest_path(filtered_graph, source\u001b[38;5;241m=\u001b[39mcurrent_node, target\u001b[38;5;241m=\u001b[39mnext_node)\n\u001b[1;32m     18\u001b[0m path_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(path)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Update the shortest walk if a shorter path is found\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/networkx/classes/backends.py:148\u001b[0m, in \u001b[0;36m_dispatch.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m NetworkXNotImplemented(\n\u001b[1;32m    146\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not implemented by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m             )\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/networkx/algorithms/shortest_paths/generic.py:171\u001b[0m, in \u001b[0;36mshortest_path\u001b[0;34m(G, source, target, weight, method)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# Find shortest source-target path.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 171\u001b[0m         paths \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mbidirectional_shortest_path(G, source, target)\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdijkstra\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    173\u001b[0m         _, paths \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mbidirectional_dijkstra(G, source, target, weight)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/networkx/algorithms/shortest_paths/unweighted.py:237\u001b[0m, in \u001b[0;36mbidirectional_shortest_path\u001b[0;34m(G, source, target)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m G \u001b[38;5;129;01mor\u001b[39;00m target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m G:\n\u001b[1;32m    236\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither source \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or target \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not in G\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNodeNotFound(msg)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# call helper to do the real work\u001b[39;00m\n\u001b[1;32m    240\u001b[0m results \u001b[38;5;241m=\u001b[39m _bidirectional_pred_succ(G, source, target)\n",
            "\u001b[0;31mNodeNotFound\u001b[0m: Either source Leonard Lipovich or target Yishai Shimoni is not in G"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "\n",
        "def shortest_ordered_walk(graph, authors_a, initial_node, end_node, N):\n",
        "    # Filter the graph to consider only the top N authors\n",
        "    filtered_graph = filter_graph_by_top_N(graph, N)\n",
        "    \n",
        "    # Initialize variables\n",
        "    shortest_walk = None\n",
        "    shortest_path_length = float('inf')\n",
        "    \n",
        "    # Iterate through all possible pairs (a_j, a_n)\n",
        "    for i in range(len(authors_a) - 1):\n",
        "        current_node = authors_a[i]\n",
        "        next_node = authors_a[i + 1]\n",
        "        \n",
        "        # Find the shortest path from current_node to next_node\n",
        "        path = nx.shortest_path(filtered_graph, source=current_node, target=next_node)\n",
        "        path_length = len(path)\n",
        "        \n",
        "        # Update the shortest walk if a shorter path is found\n",
        "        if path_length < shortest_path_length:\n",
        "            shortest_walk = path\n",
        "            shortest_path_length = path_length\n",
        "    \n",
        "    return shortest_walk\n",
        "\n",
        "def filter_graph_by_top_N(graph, N):\n",
        "    # For demonstration purposes, let's consider the top N nodes based on degree centrality\n",
        "    top_nodes = sorted(graph.nodes(), key=lambda x: graph.degree(x), reverse=True)[:N]\n",
        "    \n",
        "    filtered_graph = graph.subgraph(top_nodes)\n",
        "    \n",
        "    return filtered_graph\n",
        "\n",
        "result = shortest_ordered_walk(sub_collaboration_graph, authors_a=['Leonard Lipovich', 'Yishai Shimoni', 'Marco Roncador', 'Naoko Takahashi Tagami', 'Giuseppe Jurman', 'Albin Sandelin', 'Masanori Suzuki', 'Linda M. Van Den Berg'], initial_node='Boris Lenhard', end_node='Anita Schwegmann', N=100)\n",
        "print(\"Shortest Ordered Walk:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "538a0504",
      "metadata": {},
      "source": [
        "Premise: in ChatGPT chat, we were required to implement the code following the Functionality 3 guidelines described as above.\n",
        "\n",
        "The answer is obviously not suitable in our case since ChatGPT does not propose in any way to follow the *start node* and *end nodes* but tries to take the *shortest path* in the proposed *sequence*. In addition, as in the example above, it fails to handle whether a node is missing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c35e14b",
      "metadata": {
        "id": "2c35e14b"
      },
      "source": [
        "#### Functionality 4 - Disconnecting Graphs\n",
        "\n",
        "##### Input:\n",
        "\n",
        "- The graph data\n",
        "- authorA: a paper to which will relate sub-graph G_a\n",
        "- authorB: a paper to which will relate sub-graph G_b\n",
        "- *N*: denoting the top *N* authors that their data should be considered\n",
        "\n",
        "##### Output:\n",
        "\n",
        "- The minimum number of edges (by considering their weights) required to disconnect the original graph in two disconnected subgraphs: G_a and G_b."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "77cafd97",
      "metadata": {
        "id": "77cafd97"
      },
      "outputs": [],
      "source": [
        "sys.path.append(\"funct\")\n",
        "from functionality4 import functionality_4\n",
        "\n",
        "#We selected N = 100\n",
        "N_sel = 100\n",
        "\n",
        "#If we put 0 in 'start' and 'target' we selected two random authors\n",
        "removed_edges = functionality_4(sub_collaboration_graph, start=0, target=0, N=N_sel)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc496c69",
      "metadata": {
        "id": "bc496c69"
      },
      "source": [
        "#### Functionality 5 - Extracting Communities\n",
        "\n",
        "##### Input:\n",
        "\n",
        "- The graph data\n",
        "- *N*: denoting the top *N* papers that their data should be considered\n",
        "- Paper_1: denoting the name of one of the papers\n",
        "- Paper_2: denoting the name of one of the papers\n",
        "\n",
        "##### Output:\n",
        "\n",
        "- The minimum number of edges that should be removed to form communities\n",
        "- A list of communities, each containing a list of papers that belong to them.\n",
        "- Whether the Paper_1 and Paper_2 belongs to the same community."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d257005a",
      "metadata": {
        "id": "d257005a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n#number of nodes we want to analyze\\nn=2\\n\\n#subsample = random.sample(list(sub_citation_graph.nodes), n)\\nprint(\"Papers we want:\", subsample[0], subsample[1])\\n\\n\\ndef functionality_5(graph, p1, p2):\\n    #We need an undirected graph\\n    graph_copy = graph.to_undirected()\\n\\n    if nx.number_connected_components(graph_copy) > 1:\\n        print(\"The graph is not full connected!\")\\n        return 0, [], False\\n    \\n    #Calculate the initial number of edges\\n    num_edges_start = graph_copy.number_of_edges()\\n\\n    #Calculate betweenness centrality for each edge\\n    edge_betweenness = nx.edge_betweenness_centrality(graph_copy)\\n\\n    # Sort edges by betweenness centrality in descending order\\n    sorted_edges = sorted(edge_betweenness.items(), key=lambda x: x[1], reverse=True)\\n\\n    #Remove edges until the graph has more than one connected component \\n    while nx.number_connected_components(graph_copy) ==1:\\n        edge_to_remove = sorted_edges.pop(0)[0]\\n        graph_copy.remove_edge(*edge_to_remove)\\n    \\n    #Find connected components as communities\\n    communities = list(nx.connected_components(graph_copy))\\n\\n    # Check if Paper_1 and Paper_2 belong to the same community\\n    same_community = any({p1,p2} <= community for community in communities)\\n\\n    #Calculate the minimum number of edges removed\\n    num_edges_end = graph_copy.number_of_edges()\\n\\n    diff = num_edges_start-num_edges_end\\n\\n    # Calculate the minimum number of edges removed\\n    return diff, communities, same_community\\n\\nmin, cm, is_cm = functionality_5(sub_citation_graph.copy(), 2152761983, 2088032561)#subsample[0], subsample[1])\\nprint(\"Edges to eliminate:\", min) \\nprint(\"Number of communities:\", len(cm))\\nprint(\"Are the paper in the same community?\", is_cm)\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#sys.path.append(\"funct\")\n",
        "#from functionality5 import functionality_5\n",
        "\n",
        "'''\n",
        "#number of nodes we want to analyze\n",
        "n=2\n",
        "\n",
        "#subsample = random.sample(list(sub_citation_graph.nodes), n)\n",
        "print(\"Papers we want:\", subsample[0], subsample[1])\n",
        "\n",
        "\n",
        "def functionality_5(graph, p1, p2):\n",
        "    #We need an undirected graph\n",
        "    graph_copy = graph.to_undirected()\n",
        "\n",
        "    if nx.number_connected_components(graph_copy) > 1:\n",
        "        print(\"The graph is not full connected!\")\n",
        "        return 0, [], False\n",
        "    \n",
        "    #Calculate the initial number of edges\n",
        "    num_edges_start = graph_copy.number_of_edges()\n",
        "\n",
        "    #Calculate betweenness centrality for each edge\n",
        "    edge_betweenness = nx.edge_betweenness_centrality(graph_copy)\n",
        "\n",
        "    # Sort edges by betweenness centrality in descending order\n",
        "    sorted_edges = sorted(edge_betweenness.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    #Remove edges until the graph has more than one connected component \n",
        "    while nx.number_connected_components(graph_copy) ==1:\n",
        "        edge_to_remove = sorted_edges.pop(0)[0]\n",
        "        graph_copy.remove_edge(*edge_to_remove)\n",
        "    \n",
        "    #Find connected components as communities\n",
        "    communities = list(nx.connected_components(graph_copy))\n",
        "\n",
        "    # Check if Paper_1 and Paper_2 belong to the same community\n",
        "    same_community = any({p1,p2} <= community for community in communities)\n",
        "\n",
        "    #Calculate the minimum number of edges removed\n",
        "    num_edges_end = graph_copy.number_of_edges()\n",
        "\n",
        "    diff = num_edges_start-num_edges_end\n",
        "\n",
        "    # Calculate the minimum number of edges removed\n",
        "    return diff, communities, same_community\n",
        "\n",
        "min, cm, is_cm = functionality_5(sub_citation_graph.copy(), 2152761983, 2088032561)#subsample[0], subsample[1])\n",
        "print(\"Edges to eliminate:\", min) \n",
        "print(\"Number of communities:\", len(cm))\n",
        "print(\"Are the paper in the same community?\", is_cm)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "373421c4",
      "metadata": {
        "id": "373421c4"
      },
      "source": [
        "### 2.2. Frontend Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AKgiiKRztTTM",
      "metadata": {
        "id": "AKgiiKRztTTM"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_menu():\n",
        "    print(\"==== Menu ====\")\n",
        "    print(\"1. Display Graph Features\")\n",
        "    print(\"2. Display Node Contribution\")\n",
        "    print(\"3. Display Shortest Ordered Path\")\n",
        "    print(\"4. Display Disconnected Graph\")\n",
        "    print(\"5. Display Communities\")\n",
        "    print(\"6. Perform Operation A\")\n",
        "    print(\"7. Perform Operation B\")\n",
        "    print(\"8. Display Statistics\")\n",
        "    print(\"9. Exit\")\n",
        "\n",
        "def get_user_choice():\n",
        "    try:\n",
        "        choice = int(input(\"Enter your choice (1-9): \"))\n",
        "        if 1 <= choice <= 9:\n",
        "            return choice\n",
        "        else:\n",
        "            print(\"Invalid choice. Enter a number between 1 and 9.\")\n",
        "            return get_user_choice()\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Enter a valid number.\")\n",
        "        return get_user_choice()\n",
        "\n",
        "def display_results(results):\n",
        "    print(\"\\n==== Results ====\")\n",
        "    print(results)\n",
        "    print(\"=================\")\n",
        "\n",
        "def get_graph_data(graph_name):\n",
        "    if graph_name == \"Graph1\":\n",
        "        G = nx.Graph()\n",
        "        # Add nodes and edges to G - replace with your actual data or logic\n",
        "        return G\n",
        "    elif graph_name == \"Graph2\":\n",
        "        G = nx.Graph()\n",
        "        # Add nodes and edges to G - replace with your actual data or logic\n",
        "        return G\n",
        "    else:\n",
        "        raise ValueError(\"Invalid graph name\")\n",
        "\n",
        "def visualization_1(graph_name):\n",
        "    G = get_graph_data(graph_name)\n",
        "\n",
        "    print(\"==== Visualization 1: Visualize Graph Features ====\")\n",
        "\n",
        "    # Call backend functionality_1\n",
        "    functionality_1(G, graph_name)\n",
        "\n",
        "    # Additional visualizations for citations and collaborations\n",
        "    # Plot distribution of citations received\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    degrees_citation = dict(G.degree())\n",
        "    plt.hist(list(degrees_citation.values()), bins=20, color='blue', alpha=0.7)\n",
        "    plt.title(\"Distribution of Citations Received by Papers\")\n",
        "    plt.xlabel(\"Degree (Number of Citations)\")\n",
        "    plt.ylabel(\"Number of Papers\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot distribution of given citations\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    in_degrees_citation = dict(G.in_degree())\n",
        "    plt.hist(list(in_degrees_citation.values()), bins=20, color='green', alpha=0.7)\n",
        "    plt.title(\"Distribution of Citations Given by Papers\")\n",
        "    plt.xlabel(\"In-Degree (Number of Citations Given)\")\n",
        "    plt.ylabel(\"Number of Papers\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot number of collaborations of the author\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    degrees_collaboration = dict(G.degree())\n",
        "    plt.hist(list(degrees_collaboration.values()), bins=20, color='orange', alpha=0.7)\n",
        "    plt.title(\"Number of Collaborations of the Author\")\n",
        "    plt.xlabel(\"Degree (Number of Collaborations)\")\n",
        "    plt.ylabel(\"Number of Authors\")\n",
        "    plt.show()\n",
        "\n",
        "def visualization_2(graph_name):\n",
        "    G = get_graph_data(graph_name)\n",
        "\n",
        "    print(\"==== Visualization 2: Visualize the Node's Contribution ====\")\n",
        "\n",
        "    # Select a node from the graph\n",
        "    node_selected = input(\"Enter the node to analyze: \")\n",
        "\n",
        "    # Call backend functionality_2\n",
        "    functionality_2(G, node_selected, graph_name)\n",
        "\n",
        "\n",
        "# Main program loop\n",
        "while True:\n",
        "    display_menu()\n",
        "    choice = get_user_choice()\n",
        "\n",
        "    if choice == 1:\n",
        "        graph_name = input(\"Enter the name of the graph (Graph1 or Graph2): \")\n",
        "        visualization_1(graph_name)\n",
        "    elif choice == 2:\n",
        "        # Implement other functionalities as needed\n",
        "        # ...\n",
        "    elif choice == 9:\n",
        "        print(\"Exiting the program. Goodbye!\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"Invalid choice. Please select a valid option.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "162046cd",
      "metadata": {
        "id": "162046cd"
      },
      "source": [
        "# 4. Command Line Question (CLQ)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f16f66",
      "metadata": {},
      "source": [
        "It is possible to find the exercise in the appropriate `CommandLine.sh` file, and the screenshot in the `CLQ_screenshots` folder "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af0b626a",
      "metadata": {
        "id": "af0b626a"
      },
      "source": [
        "# 5. Algorithmic Question (AQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25dca0da",
      "metadata": {
        "id": "25dca0da"
      },
      "source": [
        "---\n",
        "## Part A\n",
        "### 1. Implement an algorithm to solve the described mentioned problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13334898",
      "metadata": {
        "id": "13334898",
        "outputId": "beb986db-1b3a-4295-946b-68bede44fb2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14 10 1\n",
            "SWM VOL ATH VOL VOL BSK HCK BSK SWM BSK\n",
            "1\n",
            "BSK 98\n",
            "2\n",
            "ATH 14\n",
            "3\n",
            "HCK 82\n",
            "4\n",
            "HCK 9\n",
            "5\n",
            "FTB 90\n",
            "6\n",
            "ATH 52\n",
            "7\n",
            "HCK 95\n",
            "8\n",
            "TEN 85\n",
            "9\n",
            "RGB 46\n",
            "10\n",
            "SWM 16\n",
            "11\n",
            "VOL 32\n",
            "12\n",
            "SOC 41\n",
            "13\n",
            "SWM 59\n",
            "14\n",
            "SWM 34\n",
            "370\n"
          ]
        }
      ],
      "source": [
        "nms = list(map(int, input().split()))\n",
        "n = nms[0]\n",
        "m = nms[1]\n",
        "s = nms[2]\n",
        "\n",
        "sports = list(map(str, input().split()))\n",
        "assert len(sports) == m, f\"We need a set of {m} sports.\"\n",
        "assert all(len(s) == 3 and s.isalpha() and s.isupper() for s in sports), \"Every element of the list must be a 3-character string with only uppercase letters.\"\n",
        "\n",
        "athl = dict()\n",
        "\n",
        "for i in range(n):\n",
        "    ix = int(input()) #ix = int(input('id of the athlete: '))\n",
        "    athl[ix] = {} #initializing a dictionary for each athlete which\n",
        "                  #will contain as key the sport and as value the proficiency of that skill\n",
        "    for j in range(s):\n",
        "        l = list(map(str, input().split()))\n",
        "        athl[ix][str(l[0])] = int(l[1])\n",
        "\n",
        "ovr = 0  #initializing the overall score\n",
        "tkn = list() #list of athletes (ids) who are already participating for a determined sport\n",
        "for i in sports:\n",
        "    mx = 0\n",
        "    for j in range(1, n + 1):\n",
        "        if i in athl[j].keys() and athl[j][i] > mx and j not in tkn:\n",
        "            mx = athl[j][i]\n",
        "            tk = j\n",
        "    ovr += mx\n",
        "    tkn.append(tk)\n",
        "    if len(tkn) == m:\n",
        "        break\n",
        "\n",
        "print(ovr)\n",
        "\n",
        "# Input 1 expected output:  370"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3068ce0f",
      "metadata": {
        "id": "3068ce0f",
        "outputId": "ef8cadb8-70c6-4af3-be68-f76a69b88c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14 10 2\n",
            "SWM VOL ATH VOL VOL BSK HCK BSK SWM BSK\n",
            "1\n",
            "BSK 98\n",
            "HCK 12\n",
            "2\n",
            "ATH 14\n",
            "VOL 1\n",
            "3\n",
            "HCK 82\n",
            "ATH 30\n",
            "4\n",
            "HCK 9\n",
            "SWM 27\n",
            "5\n",
            "FTB 90\n",
            "HCK 50\n",
            "6\n",
            "ATH 52\n",
            "RGB 80\n",
            "7\n",
            "HCK 95\n",
            "SWM 11\n",
            "8\n",
            "TEN 85\n",
            "RGB 7\n",
            "9\n",
            "RGB 46\n",
            "SWM 30\n",
            "10\n",
            "SWM 16\n",
            "BSK 12\n",
            "11\n",
            "VOL 32\n",
            "HCK 40\n",
            "12\n",
            "SOC 41\n",
            "FTB 12\n",
            "13\n",
            "SWM 59\n",
            "TEN 82\n",
            "14\n",
            "SWM 34\n",
            "VOL 20\n",
            "399\n"
          ]
        }
      ],
      "source": [
        "nms = list(map(int, input().split()))\n",
        "n = nms[0]\n",
        "m = nms[1]\n",
        "s = nms[2]\n",
        "\n",
        "sports = list(map(str, input().split()))\n",
        "assert len(sports) == m, f\"We need a set of {m} sports.\"\n",
        "assert all(len(s) == 3 and s.isalpha() and s.isupper() for s in sports), \"Every element of the list must be a 3-character string with only uppercase letters.\"\n",
        "\n",
        "athl = dict()\n",
        "\n",
        "for i in range(n):\n",
        "    ix = int(input()) #ix = int(input('id of the athlete: '))\n",
        "    athl[ix] = {} #initializing a dictionary for each athlete which\n",
        "                  #will contain as key the sport and as value the proficiency of that skill\n",
        "    for j in range(s):\n",
        "        l = list(map(str, input().split()))\n",
        "        athl[ix][str(l[0])] = int(l[1])\n",
        "\n",
        "ovr = 0  #initializing the overall score\n",
        "tkn = list() #list of athletes (ids) who are already participating for a determined sport\n",
        "for i in sports:\n",
        "    mx = 0\n",
        "    for j in range(1, n + 1):\n",
        "        if i in athl[j].keys() and athl[j][i] > mx and j not in tkn:\n",
        "            mx = athl[j][i]\n",
        "            tk = j\n",
        "    ovr += mx\n",
        "    tkn.append(tk)\n",
        "    if len(tkn) == m:\n",
        "        break\n",
        "\n",
        "print(ovr)\n",
        "\n",
        "# Input 2 expected output:  399"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a60aa68",
      "metadata": {
        "id": "6a60aa68"
      },
      "source": [
        "---\n",
        "### 2. What is the time complexity (the Big O notation) of your solution? Please provide a detailed explanation of how you calculated the time complexity.\n",
        "\n",
        "In the first for loop we are iterating on $n$ to initialize the dictionary for each athlete (`len(athl) = n`) and inside this loop there is another one which iterates on $s$ to take as imput all the proficiencies in each of the $s$ sport for each athlete.\n",
        "Then in the final big for loop we are iterating on the list \"sport\" which has `len(sports) = m` , while inside we iterate again on the $n$ athletes. So if we calculate the big O time complexity it will be:\n",
        "\n",
        "\\begin{align}\n",
        "O(n \\cdot s) + O (m \\cdot n) = O(n \\cdot s + m \\cdot n) = O(n \\cdot (m+s))\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c51f6650",
      "metadata": {
        "id": "c51f6650"
      },
      "source": [
        "---\n",
        "### 3. Ask ChatGPT or any other LLM chatbot tool to check your code's time complexity (the Big O notation). Compare your answer to theirs. If the two differ, which one is right? (why?)\n",
        "\n",
        "We asked ChatGPT and this was the answer:\n",
        "\n",
        "\n",
        "Let's analyze the time complexity of the provided algorithm.\n",
        "\n",
        "1. **Input Reading:**\n",
        "   - Reading the number of athletes (`n`), athletes to be chosen (`m`), and the number of sports skills per athlete (`s`) takes constant time, O(1).\n",
        "\n",
        "2. **Sports Input:**\n",
        "   - Reading the list of sports skills (`sports`) takes O(s) time, where s is the number of sports skills.\n",
        "\n",
        "3. **Athlete Information Input:**\n",
        "   - Reading information about each athlete (athlete ID, sports skills, and proficiency) involves a nested loop. The outer loop runs 'n' times (number of athletes), and the inner loop runs 's' times (number of sports skills per athlete). So, the time complexity of this part is O(n ⋅ s).\n",
        "\n",
        "4. **Sport Selection Loop:**\n",
        "   - The outer loop runs 's' times (number of sports). In each iteration, there is an inner loop over all athletes ('n' athletes) to find the athlete with the highest proficiency in the current sport. In the worst case, each athlete is considered for each sport, resulting in a time complexity of O(s ⋅ n).\n",
        "\n",
        "5. **Overall Complexity:**\n",
        "   - Combining the complexities of the above steps, the overall time complexity is O(1) + O(s) + O(n ⋅ s) + O(s ⋅ n), which simplifies to O(n ⋅ s).\n",
        "\n",
        "Therefore, the big O complexity of this algorithm is O(n ⋅ s), where 'n' is the number of athletes, and 's' is the number of sports skills per athlete.\n",
        "\n",
        "\n",
        "### Giving ChatGPT the code it assumed that the length of the list of the m sports was s, so this answer is wrong!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37f4a937",
      "metadata": {
        "id": "37f4a937"
      },
      "source": [
        "---\n",
        "### 4. If you algorithm has exponential time complexity, can you provide a polynomial-time version?\n",
        "The algorithm has not exponential complexity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1248018a",
      "metadata": {
        "id": "1248018a"
      },
      "source": [
        "### 5. If S = 1, how does the time complexity of an optimal algorithm to solve this problem change?\n",
        "If we watch the answer above in point 1 we can see that if we replace s with 1 the equation would be:\n",
        "\n",
        "\\begin{align}\n",
        "O(n \\cdot 1) + O (m \\cdot n) = O(n + m \\cdot n) = O(m \\cdot n)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de44a18",
      "metadata": {
        "id": "1de44a18"
      },
      "source": [
        "---\n",
        "## Part B"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf3f224c",
      "metadata": {
        "id": "cf3f224c"
      },
      "source": [
        "### 1. Prove or disprove that the problem is NP-complete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b39e63e0",
      "metadata": {
        "id": "b39e63e0"
      },
      "source": [
        "**Problem Overview**:\n",
        "\n",
        "This problem involves two main tasks:\n",
        "\n",
        "Ensuring that a team covers all the required skills in the set $T$.\n",
        "Minimizing the total effort required for the team members to work well together.\n",
        "Proving NP-Completeness:\n",
        "To show that the problem is NP-complete, we need to demonstrate two things:\n",
        "\n",
        "1. The problem is in NP (verifiable in polynomial time):\n",
        "For each skill in the set $T$, we need to check if there is at least one team member in $V$' who possesses that skill. This verification can be done by going through the set T and the set V', and the time it takes is proportional to the product of the sizes of $T$ and $V'$, denoted as $O(T  ⋅ V')$.\n",
        "\n",
        "Secondly, we use a minimum spanning tree to check if $V'$ is minimized. This involves verifying if the tree connects all the vertices in $V'$ without forming any cycles. Additionally, we check if the total weight of the tree is less than or equal to any other possible form of $G[V']$. Kruskal's algorithm can be employed for this, and its time complexity is $O(V' log V)$.\n",
        "\n",
        "Both of these verification steps take polynomial time, confirming that the problem is in NP.\n",
        "\n",
        "2. The problem is NP-Hard:\n",
        "This is typically demonstrated by reducing a known NP-complete problem to the given problem, but this part is not explicitly mentioned in the provided text.\n",
        "In summary, the problem is in NP because we can verify a solution in polynomial time, but the proof of NP-hardness is not explicitly discussed in the provided text.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c2ab2f",
      "metadata": {
        "id": "18c2ab2f"
      },
      "source": [
        "### 2. Write a heuristic in order to approximate the best solution for this problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b5c54d8",
      "metadata": {
        "id": "5b5c54d8"
      },
      "source": [
        "1. Set up a Team Subset (V'):\n",
        "    Start with an empty group of individuals, which we'll call $V'$.\n",
        "    \n",
        "\n",
        "2. Skill-Based Team Building:\n",
        "    For each skill in the required set of skills ($T$), pick an individual (vertex $v$) who has that skill. Choose the one that has the least cumulative effort in working with individuals already in the team ($V'$). If there are multiple individuals with the same skill, prioritize the one that adds the least additional effort.\n",
        "    \n",
        "    \n",
        "\n",
        "3. Build the Collaboration Graph ($G$):\n",
        "    Create an undirected weighted graph ($G$) where the nodes represent individuals in the team subset ($V'$) and the edges represent pairs of individuals. The weights on the edges represent the effort required to work together based on their skills.\n",
        "     \n",
        "     \n",
        "\n",
        "4. Complete the Team (V'):\n",
        "    Continue the process until all the required skills in $T$ are covered by individuals in the team subset ($V'$).\n",
        "    \n",
        "\n",
        "\n",
        "5. Apply Kruskal's Algorithm:\n",
        "    Use the Kruskal algorithm, as mentioned before, to identify the subset of edges in the graph ($E'$) that connect individuals in the team subset ($V'$) with the least overall weight.\n",
        "    \n",
        "    \n",
        "\n",
        "4. Resulting Collaboration Team:\n",
        "    The final team subset ($V'$) and the selected edges ($E'$) form a connected group of individuals who collectively cover all the required skills with minimal effort in working together.\n",
        "    \n",
        "    \n",
        "    \n",
        "This approach ensures that the team is composed of individuals who not only possess the necessary skills but also collaborate efficiently based on their skill compatibility.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0d2e7ea",
      "metadata": {
        "id": "f0d2e7ea"
      },
      "source": [
        "### 3. What is the time complexity of your solution?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1479a4ea",
      "metadata": {
        "id": "1479a4ea"
      },
      "source": [
        "Since the first step has time-complexity $O(T ⋅ V)$ and the second step has $O(V' log V)$ and in this type of problem $T$ is most of the times larger than $log V$, the complexity will be $O(T ⋅ V)$."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
